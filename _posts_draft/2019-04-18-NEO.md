---
layout: post
title: "Sonification of Near Earth Objects"
date: 2019-04-18 22:00:00 +0200
categories: Sonification
author: Jørgen Nygård Varpe, Guy Sion, Eigil Aandahl
image: /assets/img/varpe/NEO.jpg   
excerpt: "As a part of a two-week workshop in the Sonification and Sound design course, we were divided into groups to work on the development of a self-chosen sonification project. We had a couple of hours each day for three days allocated for development, and we chose to explore how to design and build an auditory model of Near-Earth Objects (NEO) with data collected from NASA."
Keywords: Sonification, Node.js, Max/MSP, JSON, NASA API
---

<figure>
<img src="/assets/img/varpe/NEO.jpg" width = "50%" align="center" />
</figure>

__Authors:__ Eigil Aandahl, Guy Sion and Jørgen Varpe

## Introduction

As a part of a two-week workshop in the Sonification and Sound design course, we were divided into groups to work on the development of a self-chosen sonification project. We had a couple of hours each day for three days allocated for development, and we chose to explore how to design and build an auditory model of Near-Earth Objects (NEO) with data collected from NASA.

As they orbit the Sun, NEOs can occasionally approach close to Earth. A “close” passage astronomically can be very far away in human terms: millions or even tens of millions of kilometers. These immense distances in space are hard to grasp for us living on earth. We intend to make future NEO events accessible for people by turning it into a auditory composition. The user of the system will be able to select a timespan into the future, and then listen to a composition generated from the data. As the selected timespan could be a year or more, we decided to scale one year of events down to 6 minutes (30 seconds per month). As a user of the system, you will be listening to the events happening over time with physical parameters like velocity, magnitude and distance of the NEO shaping the sound of the object.

## System

For our project, we chose to work with data provided by the SBDB Close-Approach Data API (https://ssd-api.jpl.nasa.gov/doc/cad.html). This API contains a database with all the future NEOs, and we are able to sort and filter the data with query parameters when building the URL.

We decided to use Max/MSP together with Node for Max. Node for Max was used for data retrieval and extraction, and Max/MSP for data processing and sound generation. We have been able to retrieve the selected data from the API using HTTPS requests. The parameters we extracted from the data were time and date, velocity, magnitude and distance. We then sent them as arrays to Max/MSP for further processing.

The sound from our system gets generated as each entry in the dataset gets passed to a synth voice in Max/MSP. Here, the distance and velocity gets used to create a model of the sound passing by, creating a doppler effect, much like the sound of a car going past at high speed. The magnitude of the object gets mapped to the pitch of an oscillator, creating a single tone for each object.

Here you can hear an example of how the data sounds through our system:

<figure align="middle">
<audio controls>
  <source src="https://raw.githubusercontent.com/MCT-master/mct-master.github.io/master/assets/sounds/neosounds.mp3" type="audio/mp3" volume="0.2">
  Your browser does not support the audio element.
</audio>
  <figcaption>Audio demo of NEO sounds</figcaption>
</figure>


## Reflection

The main challenges for the project were the use of the unfamiliar tool Node for Max and the data file (JSON) formatted in a way that made it difficult to work within Max/MSP. We view the presentation of a clear dataset, being able to implement the data with Node.js and developing the sound generator within Max/MSP, as our main achievements. In the end, we made the system work mostly as intended, resulting in sound from data.

Please take a look at our project files in the <a href="https://github.com/MeltingPlanet/SonifiGroupProj" target="_blank">Code repository</a>.
